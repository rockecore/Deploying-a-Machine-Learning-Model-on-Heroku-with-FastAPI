{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292875fe",
   "metadata": {},
   "source": [
    "# Unit Testing for Slice-Validation Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28f54f",
   "metadata": {},
   "source": [
    "foo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    return \"Hello world!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ef559",
   "metadata": {},
   "source": [
    "test_foo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a481fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .foo import foo\n",
    "\n",
    "def test_foo():\n",
    "    foo_result = foo()\n",
    "\n",
    "    expected_foo_result = \"Hello world!\"\n",
    "    assert foo_result == expected_foo_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe17b7",
   "metadata": {},
   "source": [
    "test_slice.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ace63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def data():\n",
    "    \"\"\" Simple function to generate some fake Pandas data.\"\"\"\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3],\n",
    "            \"numeric_feat\": [3.14, 2.72, 1.62],\n",
    "            \"categorical_feat\": [\"dog\", \"dog\", \"cat\"],\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def test_data_shape(data):\n",
    "    \"\"\" If your data is assumed to have no null values then this is a valid test. \"\"\"\n",
    "    assert data.shape == data.dropna().shape, \"Dropping null changes shape.\"\n",
    "\n",
    "\n",
    "def test_slice_averages(data):\n",
    "    \"\"\" Test to see if our mean per categorical slice is in the range 1.5 to 2.5.\"\"\"\n",
    "    for cat_feat in data[\"categorical_feat\"].unique():\n",
    "        avg_value = data[data[\"categorical_feat\"] == cat_feat][\"numeric_feat\"].mean()\n",
    "        assert (\n",
    "            2.5 > avg_value > 1.5\n",
    "        ), f\"For {cat_feat}, average of {avg_value} not between 2.5 and 3.5.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1850a1f",
   "metadata": {},
   "source": [
    "## Exercise Solution: Data Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42fad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./iris.csv\")\n",
    "\n",
    "\n",
    "def slice_iris(df, feature):\n",
    "    \"\"\" Function for calculating descriptive stats on slices of the Iris dataset.\"\"\"\n",
    "    for cls in df[\"class\"].unique():\n",
    "        df_temp = df[df[\"class\"] == cls]\n",
    "        mean = df_temp[feature].mean()\n",
    "        stddev = df_temp[feature].std()\n",
    "        print(f\"Class: {cls}\")\n",
    "        print(f\"{feature} mean: {mean:.4f}\")\n",
    "        print(f\"{feature} stddev: {stddev:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "slice_iris(df, \"septal_length\")\n",
    "slice_iris(df, \"septal_width\")\n",
    "slice_iris(df, \"petal_length\")\n",
    "slice_iris(df, \"petal_width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d69f79",
   "metadata": {},
   "source": [
    "## aequitas_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001068c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install aequitas==0.42 pandas==1.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8020039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "import aequitas.plot as ap\n",
    "\n",
    "# Enable Pandas to display dataframes without restriction.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992495f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data and take an initial look at it.\n",
    "df = pd.read_csv(\"data/compas_for_aequitas.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de93904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Race is our protected class that we will be exploring.\n",
    "df[\"race\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the races that have very little data in this data.\n",
    "df = df[~df[\"race\"].isin([\"Asian\", \"Native American\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50ddfe",
   "metadata": {},
   "source": [
    "### Create Crosstab\n",
    "\n",
    "Create the crosstab that forms the basis for all the subsequent analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29212c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = Group()\n",
    "xtab, _ = group.get_crosstabs(df)\n",
    "\n",
    "xtab.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d950826",
   "metadata": {},
   "source": [
    "### Compute Bias\n",
    "\n",
    "We calculate the bias vs. a predefined group we manually set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e201c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = Bias()\n",
    "bias_df = bias.get_disparity_predefined_groups(xtab,\n",
    "                                               original_df=df,\n",
    "                                               ref_groups_dict={\"race\": \"Caucasian\", \"sex\": \"Male\", \"age_cat\": \"25 - 45\"},\n",
    "                                               alpha=0.05,\n",
    "                                               mask_significance=True)\n",
    "bias_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias.get_disparity_major_group(xtab,\n",
    "                               original_df=df,\n",
    "                               alpha=0.05,\n",
    "                               mask_significance=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness = Fairness()\n",
    "fairness_df = fairness.get_group_value_fairness(bias_df)\n",
    "fairness_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480409e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_fairness = fairness.get_overall_fairness(fairness_df)\n",
    "print(overall_fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b83f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['fpr', 'fnr', 'for']\n",
    "disparity_tolerance = 1.25\n",
    "\n",
    "ap.summary(bias_df, metrics, fairness_threshold=disparity_tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c710a7",
   "metadata": {},
   "source": [
    "## Exercise: Aequitas\n",
    "\n",
    "In this exercise, you will use Aequitas to investigate the potential bias in a model/data set.\n",
    "\n",
    "* We'll use the Car Evaluation Data Set from the UCI Machine Learning Repository, a notebook that trains a logistic regression model to determine the car's acceptability is provided.\n",
    "* Using Aequitas, determine if the model contains bias. For simplicity, from Aequitas' Fairness class obtain the results of the get_overall_fairness method which returns a dictionary with Yes/No result for \"Unsupervised Fairness\", \"Supervised Fairness\" and \"Overall Fairness\".\n",
    "* Lastly, use the aequitas.plotting.Plot module and compute the summary on fpr, fnr, and for with a 1.25 fairness_threshold.\n",
    "* You can draw inspiration from examples present here: https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba12a5d",
   "metadata": {},
   "source": [
    "The data from this exercise comes from the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Car+Evaluation For more details on the data set see the included documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that Aequitas dependency is installed\n",
    "import sys\n",
    "!{sys.executable} -m pip install aequitas==0.42 pandas==1.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from aequitas.plotting import Plot\n",
    "ap = Plot()\n",
    "import pandas as pd\n",
    "\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias \n",
    "from aequitas.fairness import Fairness\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, label_binarize, LabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I manually added the headers to the data set.\n",
    "df = pd.read_csv(\"./car.csv\")\n",
    "\n",
    "# We'll modify the data to make it a binary problem of acceptable or unacceptable car.\n",
    "df = df.where(df != 'good', 'acc')\n",
    "df = df.where(df != 'vgood', 'acc')\n",
    "\n",
    "y = df.pop('car')\n",
    "X = df\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d12dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "\n",
    "# Use this later to construct the DataFrame Aequitas requires.\n",
    "df_aq = X_test.copy()\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "X_train = ohe.fit_transform(X_train.values)\n",
    "X_test = ohe.transform(X_test.values)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train = label_binarize(y_train.values, classes=['unacc', 'acc']).ravel()\n",
    "y_test = label_binarize(y_test.values, classes=['unacc', 'acc']).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a8fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "scores = lr.predict_proba(X_test)\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, pred)\n",
    "print(f\"F1 score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1d264",
   "metadata": {},
   "source": [
    "* Construct the dataframe that Aequitas will use.\n",
    "* You can draw inspiration from examples present here: https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Aequitas.\n",
    "# Summarize: Aequitas classes provides a few functions that provide a high level summary of fairness and disparity, such as \n",
    "# plot_fairness_group_all()\n",
    "# plot_fairness_disparity_all()\n",
    "# plot_disparity_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbe0f7",
   "metadata": {},
   "source": [
    "# Write a model card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c208c7",
   "metadata": {},
   "source": [
    "Model Details: \n",
    "Justin C Smith created the model. It is logistic regression using the default hyperparameters in scikit-learn 0.24.2.\n",
    "\n",
    "Intended Use: \n",
    "This model should be used to predict the acceptability of a car based off a handful of attributes. The users are prospective car buyers.\n",
    "\n",
    "Metrics: \n",
    "The model was evaluated using F1 score. The value is 0.8960.\n",
    "\n",
    "Data: \n",
    "The data was obtained from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Car+Evaluation). The target class was modified from four categories down to two: \"unacc\" and \"acc\", where \"good\" and \"vgood\" were mapped to \"acc\".\n",
    "\n",
    "The original data set has 1728 rows, and a 75-25 split was used to break this into a train and test set. No stratification was done. To use the data for training a One Hot Encoder was used on the features and a label binarizer was used on the labels.\n",
    "\n",
    "Bias: \n",
    "According to Aequitas bias is present at the unsupervised and supervised level. This implies an unfairness in the underlying data and also unfairness in the model. From Aequitas summary plot we see bias is present in only some of the features and is not consistent across metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de8b845",
   "metadata": {},
   "source": [
    "# Data and Model Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74c198",
   "metadata": {},
   "source": [
    "DVC's commands are designed to be very similar to Git's.\n",
    "\n",
    "Initializing a project using git init or dvc init.\n",
    "\n",
    "To add code or data use git add or dvc add, respectively. Typically, after a dvc add it will then prompt you to git commit the corresponding .dvc file that has been generated. There is a dvc commit but it is not used in the same way as with git commit.\n",
    "\n",
    "Lastly, there is git push and git pull and their equivalents of dvc push and dvc pull. DVC's push and pull are for uploading and downloading data from your remote store specified in in the dvc configuration, whereas git is for sending changes to your remote repository or bringing in any changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db577a",
   "metadata": {},
   "source": [
    "## Tracking Data with DVC\n",
    "\n",
    "To create it, simply make the folder and tell DVC it is your remote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /local/remote\n",
    "dvc remote add -d localremote /local/remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a5869",
   "metadata": {},
   "source": [
    "## Tracking Data Locally with DVC\n",
    "\n",
    "Set up the repository and local remote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "git init\n",
    "dvc init\n",
    "mkdir ../local_remote\n",
    "dvc remote add -d localremote ../local_remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277adfd",
   "metadata": {},
   "source": [
    "The code to generate a csv called exercise_func.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_ids(id_count: str) -> None:\n",
    "    \"\"\" Generate a list of IDs and save it as a csv.\"\"\"\n",
    "    ids = [i for i in range(int(id_count))]\n",
    "    df = pd.DataFrame(ids)\n",
    "    df.to_csv(\"./id.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_ids(sys.argv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa640f2",
   "metadata": {},
   "source": [
    "And then push the data and changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./exercise_func.py 10\n",
    "dvc add id.csv\n",
    "git add .gitignore id.csv.dvc\n",
    "git commit -m \"Initial commit of tracked sample.csv\"\n",
    "dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9fe27a",
   "metadata": {},
   "source": [
    "## Tracking Data Remotely with DVC\n",
    "\n",
    "Install the Google Drive dependencies for DVC using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge dvc-gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4162a",
   "metadata": {},
   "source": [
    "Add the Google Drive remote using the unique identifier found in the URL of your Drive folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc remote add driveremote gdrive://UNIQUE_IDENTIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66cd031",
   "metadata": {},
   "source": [
    "At this point you will receive a pop up to authenticate with Google Drive. Complete the authentication in the browser and copy the provided code into the command line prompt.\n",
    "\n",
    "Then push using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9031008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc push --remote driveremote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2140eec",
   "metadata": {},
   "source": [
    "or you can now set the Google Drive remote as your default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd843b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc remote default newremote\n",
    "dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0de0a",
   "metadata": {},
   "source": [
    "## Pipelines with DVC\n",
    "\n",
    "Modify train.py as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from yaml import CLoader as Loader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "with open(\"./params.yaml\", \"rb\") as f:\n",
    "    params = yaml.load(f, Loader=Loader)\n",
    "\n",
    "X = np.loadtxt(\"X.csv\")\n",
    "y = np.loadtxt(\"y.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=23\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(C=params[\"C\"])\n",
    "lr.fit(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "preds = lr.predict(X_test.reshape(-1, 1))\n",
    "f1 = f1_score(y_test, preds)\n",
    "print(f\"F1 score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c3aaa",
   "metadata": {},
   "source": [
    "This assumes a param.yaml in the working directory with a single line of C: 1.0, or whichever value you choose.\n",
    "\n",
    "To create the prepare stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a034c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc run -n prepare -d fake_data.csv -d prepare.py -o X.csv -o y.csv python ./prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c5c58",
   "metadata": {},
   "source": [
    "And to create the train stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c13b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc run -n train -d X.csv -d y.csv -d train.py -p C python ./train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240708b6",
   "metadata": {},
   "source": [
    "Now that we have a robust and reproducible pipeline built with DVC we are almost ready to use it to track experiments. But first we need to add metrics to our pipeline so that we have something to compare across experiments.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61aed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc run -n evaluate \\\n",
    "          -d validate.py -d model.pkl \\\n",
    "          -M validation.json \\\n",
    "          python validate.py model.pkl validation.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603029d9",
   "metadata": {},
   "source": [
    "where we have now included a metric in our stage. A similar process can be used to include plots as part of a stage.\n",
    "\n",
    "We are now ready to set up experiments. Simply use dvc exp run and specify the relevant parameters to run an experiment. Experiments can be compared using dvc exp diff or dvc exp show. Each experiment is given a unique name that can be used for management and ultimately choosing which experiment to keep as we prepare our model for deployment. Don't forget to commit the best experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6dff4",
   "metadata": {},
   "source": [
    "# CI/CD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c0c2a",
   "metadata": {},
   "source": [
    "## Setting up GitHub Actions\n",
    "\n",
    "The first action I've chosen is the Python Application Action, part of which we saw in the lesson. This Action installs Python and the requirements for your application (if there are any). Lastly it runs flake8 and pytest -- the build fails if either a test fails or certain flake8 errors are hit.\n",
    "\n",
    "In principle, one should run flake8 and pytest before you commit your code since that can be faster than waiting on the automated build process but this piece of automation ensures any contributors to the code also pass flake8 and pytest and they get checked in case you don't run them yourself. It also ensures that both of these pass when the code is built in a clean environment.\n",
    "\n",
    "python-app.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This workflow will install Python dependencies, run tests and lint with a single version of Python\n",
    "# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions\n",
    "\n",
    "name: Python application\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  build:\n",
    "\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python 3.9\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: 3.9\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install flake8 pytest\n",
    "        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n",
    "    - name: Lint with flake8\n",
    "      run: |\n",
    "        # stop the build if there are Python syntax errors or undefined names\n",
    "        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n",
    "        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n",
    "        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n",
    "    - name: Test with pytest\n",
    "      run: |\n",
    "        pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9904809",
   "metadata": {},
   "source": [
    "The second Action is a scheduled job that uses a Unix time-based job scheduler called cron. This Action simply marks issues and pull requests as stale if they have no activity. If you go to stale.yaml's repo then you will see that there are many more options than shown in the default YAML file.\n",
    "\n",
    "This Action is particularly useful for large projects, e.g. open-source projects, since it helps with the maintenance of issues and pull requests. It makes it easier to track what is current or not, and what may need attention without manually combing through the issues and pull requests.\n",
    "\n",
    "stale.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce19baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: Mark stale issues and pull requests\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "  - cron: \"30 1 * * *\"\n",
    "\n",
    "jobs:\n",
    "  stale:\n",
    "\n",
    "    runs-on: ubuntu-latest\n",
    "    permissions:\n",
    "      issues: write\n",
    "      pull-requests: write\n",
    "\n",
    "    steps:\n",
    "    - uses: actions/stale@v3\n",
    "      with:\n",
    "        repo-token: ${{ secrets.GITHUB_TOKEN }}\n",
    "        stale-issue-message: 'Stale issue message'\n",
    "        stale-pr-message: 'Stale pull request message'\n",
    "        stale-issue-label: 'no-issue-activity'\n",
    "        stale-pr-label: 'no-pr-activity'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c854a7b",
   "metadata": {},
   "source": [
    "## Heroku Fundamentals\n",
    "\n",
    "### Setting up CD\n",
    "\n",
    "* Navigate to the Heroku dashboard and select the button for \"New\" then select Create new app.\n",
    "* After creating an you will be brought to the deploy screen. Select GitHub as the deployment method, then search for a repository on your GItHub to connect. In the automatic deploys section select \"Wait for CI to pass before deploy\" and then click the \"Enable Automatic Deploys\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945fb25",
   "metadata": {},
   "source": [
    "# API Deployment with FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719e2b1",
   "metadata": {},
   "source": [
    "We can define a very simple API where each input is simply an int and it contains a path, query and body as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "class Value(BaseModel):\n",
    "    value: int\n",
    "\n",
    "\n",
    "@app.post(\"/{path}\")\n",
    "async def exercise_function(path: int, query: int, body: Value):\n",
    "    return {\"path\": path, \"query\": query, \"body\": body}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61813516",
   "metadata": {},
   "source": [
    "We can use test_bar.py to test the code in bar.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f79804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_bar.py\n",
    "\n",
    "import json\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "from bar import app\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "\n",
    "def test_post():\n",
    "    data = json.dumps({\"value\": 10})\n",
    "    r = client.post(\"/42?query=5\", data=data)\n",
    "    print(r.json())\n",
    "    assert r.json()[\"path\"] == 42\n",
    "    assert r.json()[\"query\"] == 5\n",
    "    assert r.json()[\"body\"] == {\"value\": 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f7459",
   "metadata": {},
   "source": [
    "## Local API Testing\n",
    "\n",
    "Stylistically, I split each test into separate functions. Some people will put all tests of a single function/method in a single test function, others will break it out. I find that the approach below facilitates rapid identification of what exactly is failing when a test breaks. Assuming the app is located in foo.py then for test_foo.py I have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_foo.py\n",
    "\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "from foo import app\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "\n",
    "def test_get_path():\n",
    "    r = client.get(\"/items/42\")\n",
    "    assert r.status_code == 200\n",
    "    assert r.json() == {\"fetch\": \"Fetched 1 of 42\"}\n",
    "\n",
    "\n",
    "def test_get_path_query():\n",
    "    r = client.get(\"/items/42?count=5\")\n",
    "    assert r.status_code == 200\n",
    "    assert r.json() == {\"fetch\": \"Fetched 5 of 42\"}\n",
    "\n",
    "\n",
    "def test_get_malformed():\n",
    "    r = client.get(\"/items\")\n",
    "    assert r.status_code != 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ad7cd",
   "metadata": {},
   "source": [
    "Running Local API Tests\n",
    "\n",
    "To run the tests, invoke pytest at the command line. With the supplied foo.py and test_foo.py it should yield these results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4fe55",
   "metadata": {},
   "source": [
    "## Exploring Heroku's CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58306fec",
   "metadata": {},
   "source": [
    "Steps\n",
    "* Install the Heroku CLI. I used the Ubunutu installation instructions: curl https://cli-assets.heroku.com/install.sh | sh\n",
    "* Type 'heroku' to see the full list of commands.\n",
    "* To create an app with a specific name and to specify it as Python app use: heroku create name-of-the-app --buildpack heroku/python\n",
    "    * Note that manually specifying it as a Python app is not necessary. Heroku will automatically try and detect the language of your app. In the case of Python it searches for either a requirements.txt or setup.py.\n",
    "* We can view our apps buildpacks using heroku buildpacks --app name-of-the-app.\n",
    "* Now we'll initiate our folder as a git repository and commit it so we can connect it to our new Heroku app.\n",
    "    * git init\n",
    "    * git add *\n",
    "    * git commit -m \"Initial commit.\"\n",
    "* Connect the repo to our new app: heroku git:remote --app name-of-the-app.\n",
    "    * The app will launch after a few moments.\n",
    "* Enter into the Heroku VM using: heroku run bash --app name-of-the-app.\n",
    "    * There is not much to see here. Explore the various Unix commands such as pwd, and ls. Doing 'ls .. reveals many of the standard folders one would expect in a Unix environment. Note this is very lightweight, not even vi is included!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957ac71",
   "metadata": {},
   "source": [
    "## Live API Testing with the requests Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8308cc3",
   "metadata": {},
   "source": [
    "We will test our API one final time now that it is fully deployed. To do this we will use the requests module. This module makes it painless to POST, GET, DELETE, etc. with any API.\n",
    "\n",
    "Using a few lines of code we can POST to an endpoint and retrieve both the status code and the resulting JSON in the response object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post('/url/to/query/')\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa2af4",
   "metadata": {},
   "source": [
    "The requests module is expansive, but a few helpful additions to the bare HTTP methods include authentication and passing in data as JSON objects using the auth and data parameters, respectively. E.g.:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e867a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('/url/to/query/', auth=('usr', 'pass'), data=json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ad130",
   "metadata": {},
   "source": [
    "Trying out Live APIs\n",
    "\n",
    "A fun way to get experience with the requests module is to query a live API! There is an extensive list of free and public APIs here. Many of these do not even require an authorization key, though getting access to one that needs a key is also good practice!\n",
    "\n",
    "Note that not all APIs are created equally and some have way significantly more documentation than others. For instance the Art Institute of Chicago's API has extensive documentation. Some APIs even have their documentation using Swagger like you saw with FastAPI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c6be01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
